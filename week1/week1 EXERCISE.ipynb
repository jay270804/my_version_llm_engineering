{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown, update_display\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "\n",
    "def set_prompt(question):\n",
    "    return [\n",
    "        {\n",
    "            'role':'system',\n",
    "            'content':'You are computer science and programming tutor,\\\n",
    "            your job is give explanations or solution along with easy understandable examples,\\\n",
    "            give response as markdown format especially proper markdown to highlight code'\n",
    "        },\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':question\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Api key is Okay\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print('Api key is Okay')\n",
    "else:\n",
    "    print('something wrong with apikey')\n",
    "openai = OpenAI()\n",
    "\n",
    "def printer(data):\n",
    "    display(Markdown(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's break down the code you've provided to understand its purpose and functionality.\n",
       "\n",
       "### Code Explanation\n",
       "\n",
       "The line of code in question is:\n",
       "\n",
       "python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "\n",
       "#### Components Breakdown:\n",
       "\n",
       "1. **Set Comprehension**: \n",
       "   The expression inside the curly braces `{}` is a **set comprehension**. A set comprehension creates a set, which is a collection type that holds only unique items. The expression generates a set of authors from the `books` collection:\n",
       "   \n",
       "   - **`book.get(\"author\")`**: This retrieves the value associated with the key `\"author\"` from each `book` dictionary in the `books` list. If the key does not exist, `get` will return `None`.\n",
       "   \n",
       "2. **Looping through `books`**:\n",
       "   The `for book in books` part goes through each `book` in the `books` iterable (which is assumed to be a list of dictionaries).\n",
       "\n",
       "3. **Conditional Filtering**:\n",
       "   The `if book.get(\"author\")` portion ensures that only books with an existing author (non-`None` or non-empty strings) are included in the set. This means you won't get any empty or `None` authors in your final set.\n",
       "\n",
       "4. **Yielding Values**:\n",
       "   Lastly, `yield from` is a Python statement that is typically used within a generator function. It allows the generator to yield all values from another iterable. In this case, it will yield each unique author from the set that was generated.\n",
       "\n",
       "### Full Summary\n",
       "\n",
       "1. This line creates a set of unique authors from a list of books (ignoring duplicates and any books that do not have an author).\n",
       "2. It then yields those authors one by one.\n",
       "\n",
       "### Example\n",
       "\n",
       "Here's a complete example to illustrate how it works:\n",
       "\n",
       "python\n",
       "def get_unique_authors(books):\n",
       "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "# Example books list\n",
       "books = [\n",
       "    {\"title\": \"Book One\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book Two\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book Three\", \"author\": \"Author A\"},  # Duplicate author\n",
       "    {\"title\": \"Book Four\"},  # No author\n",
       "    {\"title\": \"Book Five\", \"author\": \"Author C\"},\n",
       "]\n",
       "\n",
       "# Using the generator function\n",
       "for author in get_unique_authors(books):\n",
       "    print(author)\n",
       "\n",
       "\n",
       "### Expected Output\n",
       "In this example, the output will be:\n",
       "\n",
       "\n",
       "Author A\n",
       "Author B\n",
       "Author C\n",
       "\n",
       "\n",
       "Here, each unique author is printed once despite \"Author A\" being present twice in the list of books and \"Book Four\" being ignored as it does not have an author."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "def stream_GPT(question):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=set_prompt(question),\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = ''\n",
    "    display_handle = display(Markdown(''), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace('```', '').replace('markdown', '')\n",
    "        update_display(Markdown(response),display_id=display_handle.display_id)\n",
    "stream_GPT(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Generator Expression Explanation**\n",
       "=====================================\n",
       "\n",
       "The given code snippet is a generator expression that uses the `yield from` keyword to delegate iteration to another iterable. Let's break it down:\n",
       "\n",
       "### Code Explanation\n",
       "\n",
       "```python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "```\n",
       "\n",
       "This line of code is using several Python features:\n",
       "\n",
       "1. **Generator Expression**: The expression `{...}` is a generator expression, which is similar to a list comprehension but returns an iterator instead of a list.\n",
       "2. **`yield from`**: This keyword is used to delegate iteration to another iterable. When `yield from` encounters the first `yield` in the nested iterable, it will yield its value and then move on to the next item in the outer generator expression.\n",
       "3. **Dictionary Comprehension**: The inner `{...}` is a dictionary comprehension, which creates a new dictionary with key-value pairs based on the given conditions.\n",
       "\n",
       "### What the Code Does\n",
       "\n",
       "In summary, this code does the following:\n",
       "\n",
       "* Iterates over the `books` iterable (assuming it's a list or similar).\n",
       "* For each book, checks if the \"author\" key exists in the book's dictionary using `.get(\"author\")`. If it doesn't exist, the book is skipped.\n",
       "* For books that do have an \"author\", it yields the author's value.\n",
       "\n",
       "### Why `yield from`?\n",
       "\n",
       "Using `yield from` is beneficial when you want to write a generator function that iterates over another iterable. This approach has several advantages:\n",
       "\n",
       "* **Efficient**: Generator expressions are memory-efficient because they don't store all the values in memory at once.\n",
       "* **Flexible**: You can use `yield from` with any iterable, making it a powerful tool for complex iteration scenarios.\n",
       "\n",
       "### Example Use Case\n",
       "\n",
       "Here's an example use case where this code might be used:\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": None},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Author C\"}\n",
       "]\n",
       "\n",
       "authors = yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "print(authors)  # Output: [\"Author A\", \"Author C\"]\n",
       "```\n",
       "In this example, the code generates a list of authors from the `books` iterable using the dictionary comprehension and `yield from`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "def call_llama(question):\n",
    "    llama_response = ollama.chat(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=set_prompt(question)\n",
    "    )\n",
    "    return llama_response['message']['content']\n",
    "    \n",
    "printer(call_llama(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c1f2398-a1e3-4491-b8dc-f95cdfdd3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Can you explain me how streaming works which is causing typewriter like animation in below openai code?\n",
    "def stream_GPT(question):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=set_prompt(question),\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = ''\n",
    "    display_handle = display(Markdown(''), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace('```', '').replace('markdown', '')\n",
    "        update_display(Markdown(response),display_id=display_handle.display_id)\n",
    "\n",
    "Can I generate similiar typewriter animation when calling `llama model` using `ollama` library?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b03b3a5a-d6b3-4c50-9fec-7dcb8ee9bb1d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Streaming in OpenAI's GPT Model**\n",
       "=====================================\n",
       "\n",
       "The code you provided uses OpenAI's GPT model to create a chat completion service. The `stream=True` parameter in the `completions.create()` method allows for streaming, which enables the server to return the response one chunk at a time, rather than waiting for the entire response.\n",
       "\n",
       "**How Streaming Works**\n",
       "------------------------\n",
       "\n",
       "Here's a simplified explanation of how streaming works:\n",
       "\n",
       "1.  **Chunking**: The GPT model breaks down the response into smaller chunks, typically around 4096 characters.\n",
       "2.  **Sending chunks**: The server sends each chunk to your application as soon as it is generated.\n",
       "3.  **Receiving and processing chunks**: Your application receives each chunk, processes it (e.g., removes markdown formatting), and updates the display accordingly.\n",
       "\n",
       "**Typewriter Animation**\n",
       "------------------------\n",
       "\n",
       "The typewriter animation you see in the example code is created by updating the `response` variable on each chunk received from the server. This causes the text to appear as if it's being typed.\n",
       "\n",
       "Here's a breakdown of how this works:\n",
       "\n",
       "*   The initial response is an empty string (`''`).\n",
       "*   On each iteration, the current response is updated with the new chunk.\n",
       "*   Any markdown formatting or code blocks are removed from the response using `replace('```', '').replace('markdown', '')`.\n",
       "*   The updated response is then displayed on the screen.\n",
       "\n",
       "**Streaming in LLaMA Model Using Ollama Library**\n",
       "---------------------------------------------------\n",
       "\n",
       "To create a similar typewriter animation when calling an LLaMA model using the Ollama library, you can use a streaming approach as well. Here's an example:\n",
       "\n",
       "```python\n",
       "import ollama\n",
       "\n",
       "# Initialize the LLaMA model and chat context\n",
       "model = ollama.LlamaModel(\"LLaMA\")\n",
       "context = {}\n",
       "\n",
       "# Set up the display handle\n",
       "display_handle = ollama.display.Display()\n",
       "\n",
       "while True:\n",
       "    # Prompt the user for input\n",
       "    question = input(\"Enter a question: \")\n",
       "\n",
       "    # Create a new chat session\n",
       "    context = model.create_session(question)\n",
       "\n",
       "    # Initialize the response\n",
       "    response = \"\"\n",
       "\n",
       "    # Iterate over each chunk of the response\n",
       "    for chunk in context:\n",
       "        # Append the chunk to the response\n",
       "        response += chunk[\"text\"]\n",
       "\n",
       "        # Remove any markdown formatting or code blocks from the response\n",
       "        response = response.replace(\"```\", \"\").replace(\"# \", \"\")\n",
       "\n",
       "        # Update the display handle with the new response\n",
       "        display_handle.update(Markdown(response))\n",
       "\n",
       "    # Wait for a short period before checking again\n",
       "    import time\n",
       "    time.sleep(0.5)\n",
       "```\n",
       "\n",
       "In this example, we use an infinite loop to continuously prompt the user for input and create new chat sessions with the LLaMA model. On each iteration, we append the response chunk to the `response` variable, remove any markdown formatting or code blocks, and update the display handle with the new response.\n",
       "\n",
       "By using a streaming approach, we can achieve the same typewriter animation effect as in the original example code."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printer(call_llama(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1f05fd3-f954-465d-8624-1710dfa29d49",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! In the provided code, the concept of streaming is used to create a typewriter-like animation effect when displaying text generated by an AI model (in this case, OpenAI's models). Let's break down the code and understand how it works:\n",
       "\n",
       "### Explanation of the Code\n",
       "\n",
       "1. **Streaming Response**:\n",
       "   python\n",
       "   stream = openai.chat.completions.create(\n",
       "       model=MODEL_GPT,\n",
       "       messages=set_prompt(question),\n",
       "       stream=True\n",
       "   )\n",
       "   \n",
       "   This line initiates a streaming call to the OpenAI API. The `stream=True` argument means that the response will be sent back in chunks rather than all at once.\n",
       "\n",
       "2. **Display Setup**:\n",
       "   python\n",
       "   display_handle = display(Markdown(''), display_id=True)\n",
       "   \n",
       "   This sets up an initial display that can be updated later, using the Jupyter notebook's display functionality.\n",
       "\n",
       "3. **Processing the Stream**:\n",
       "   python\n",
       "   for chunk in stream:\n",
       "       response += chunk.choices[0].delta.content or ''\n",
       "       response = response.replace('', '').replace('', '')\n",
       "       update_display(Markdown(response), display_id=display_handle.display_id)\n",
       "   \n",
       "   - This loop processes each chunk of the response as it arrives.\n",
       "   - Each chunk's content is added to a `response` string.\n",
       "   - The display is updated in real-time with the current content of `response`, creating the typewriter effect as the AI produces text.\n",
       "\n",
       "### Creating a Similar Animation with `Ollama` Library\n",
       "\n",
       "If you want to create a similar typewriter animation effect when calling a Llama model using the `ollama` library, you can do so by implementing a similar strategy. Here's a general outline of how you could achieve this:\n",
       "\n",
       "1. **Initialize Streaming**:\n",
       "   You need a streaming option with the `ollama` library, similar to how you did it with the OpenAI API.\n",
       "\n",
       "2. **Display Content in Chunks**:\n",
       "   While you receive the chunks, concatenate them and update the display incrementally.\n",
       "\n",
       "Here’s an illustrative example of how you might implement this:\n",
       "\n",
       "python\n",
       "import ollama  # Assume Ollama provides necessary imports\n",
       "from IPython.display import display, Markdown, update_display\n",
       "\n",
       "def stream_llama(question):\n",
       "    stream = ollama.api.chat(\n",
       "        model=\"llama\",\n",
       "        messages=[{\"role\": \"user\", \"content\": question}],\n",
       "        stream=True\n",
       "    )\n",
       "\n",
       "    response = ''\n",
       "    display_handle = display(Markdown(''), display_id=True)\n",
       "\n",
       "    for chunk in stream:\n",
       "        # Assuming each chunk has a similar structure as in OpenAI's API\n",
       "        response += chunk['choices'][0]['delta']['content'] or ''\n",
       "        response = response.replace('', '').replace('', '')\n",
       "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
       "\n",
       "# Usage\n",
       "stream_llama(\"What is the future of AI?\")\n",
       "\n",
       "\n",
       "### Key Considerations\n",
       "- **Ensure Streaming is Supported**: Check if the version of the `ollama` library you are using supports streaming responses.\n",
       "- **Chunk Handling**: Ensure that the structure of the chunks matches the one expected in the loop (i.e., `chunk['choices'][0]['delta']['content']`).\n",
       "- **Display Mechanism**: This example uses a Jupyter Notebook for display. If you are working in a different environment (e.g., a web application), the display mechanisms might differ.\n",
       "\n",
       "By following the above structure, you should be able to achieve a typewriter-like animation with the `ollama` library similar to the one seen with the OpenAI GPT model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_GPT(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b77fcb0-3443-4a7f-b1a9-7f29845a4559",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Streaming and Typewriter Animation\n",
       "=====================================\n",
       "\n",
       "The typewriter-like animation in the provided OpenAI code is achieved through streaming. Streaming allows you to receive the response from the API one chunk at a time, rather than waiting for the entire response.\n",
       "\n",
       "Here's what happens when we create a stream using `openai.chat.completions.create(stream=True)`:\n",
       "\n",
       "1.  The API sends the first chunk of the response.\n",
       "2.  Our code processes this chunk and updates the display with the new text.\n",
       "3.  We receive the next chunk from the API.\n",
       "4.  We repeat steps 2-3 until we've received the entire response.\n",
       "\n",
       "This creates a seamless, continuous updating effect on the screen, similar to a typewriter animation.\n",
       "\n",
       "Streaming in Python\n",
       "--------------------\n",
       "\n",
       "In Python, you can use the `stream` module or libraries like `asyncio` and `aiohttp` to create streams. However, for simplicity, we'll focus on using the existing API methods provided by OpenAI.\n",
       "\n",
       "To achieve a similar effect when calling an `llama model` using the `transformers` library (which provides a Python interface for the Hugging Face models), you can use the following code:\n",
       "\n",
       "```python\n",
       "import torch\n",
       "from transformers import LLaMAForCausalLM, LLaMAScheduler\n",
       "\n",
       "def stream_llama(question):\n",
       "    # Initialize the model and scheduler\n",
       "    model = LLaMAForCausalLM.from_pretrained('llama')\n",
       "    scheduler = LLaMAScheduler()\n",
       "\n",
       "    # Create an iterator for streaming responses\n",
       "    response_iterator = iter(model.generate(\n",
       "        input_ids=torch.tensor([question]), \n",
       "        max_length=100, \n",
       "        num_returned_sequences=1, \n",
       "        no_repeat_ngram_size=2, \n",
       "        early_stopping=True, \n",
       "        max_model_size=1024 * 1024 * 50\n",
       "    ))\n",
       "\n",
       "    response = ''\n",
       "    for i, response in enumerate(response_iterator):\n",
       "        # Process the response and update the display (assuming display is a function that updates the UI)\n",
       "        response_text = model.decode_ids(response[0])\n",
       "        if response_text:\n",
       "            response += ' '.join(response_text) + '\\n'\n",
       "\n",
       "# Example usage\n",
       "question = \"What is the capital of France?\"\n",
       "stream_llama(question)\n",
       "```\n",
       "\n",
       "In this code:\n",
       "\n",
       "*   We use `model.generate` to create an iterator for streaming responses.\n",
       "*   The `max_length`, `num_returned_sequences`, and `early_stopping` arguments are used to control the output length, sequence generation, and stopping criteria, respectively.\n",
       "\n",
       "Note that the above example uses a simple approach to process the response. You can enhance this by adding more sophisticated text processing techniques, such as filtering or summarization.\n",
       "\n",
       "By using streaming, you can achieve a similar typewriter-like animation when calling an `llama model` using the `transformers` library."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def stream_llama(question):\n",
    "    stream = ollama.chat(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=set_prompt(question),\n",
    "        stream = True\n",
    "    )\n",
    "\n",
    "    response = ''\n",
    "    display_handle = display(Markdown(''), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content'] or ''\n",
    "        # response = response.replace('```','').replace('markdown','')\n",
    "        update_display(Markdown(response),display_id=display_handle.display_id)\n",
    "stream_llama(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea24e32c-c6c6-4f14-8aff-9ec9ccf0cb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Factory Design Pattern**\n",
       "==========================\n",
       "\n",
       "The Factory design pattern is a creational design pattern that provides an interface for creating objects without specifying the exact class of object that will be created.\n",
       "\n",
       "**Problem**\n",
       "-----------\n",
       "\n",
       "In traditional object-oriented programming (OOP), we would typically create objects by instantiating classes. However, this approach has several drawbacks:\n",
       "\n",
       "*   It can lead to tightly coupled code, where the client code is dependent on a specific class implementation.\n",
       "*   It can make it difficult to add new functionality or change existing behavior without modifying existing code.\n",
       "\n",
       "**Solution**\n",
       "------------\n",
       "\n",
       "The Factory design pattern solves these problems by introducing an abstraction layer between the client code and the concrete object classes. This abstraction allows us to create objects without specifying their exact class, decoupling the client code from specific implementations.\n",
       "\n",
       "**Key Components**\n",
       "-------------------\n",
       "\n",
       "A typical Factory design pattern consists of:\n",
       "\n",
       "*   **Factory Interface**: Defines the interface for creating objects.\n",
       "*   **Concrete Factories**: Implement the factory interface and provide specific object creation logic.\n",
       "*   **Product Classes**: Represent the actual objects being created.\n",
       "*   **Client Code**: Uses the factory interface to create objects without knowing their concrete class.\n",
       "\n",
       "**Example**\n",
       "------------\n",
       "\n",
       "Suppose we want to write a program that creates different types of vehicles (e.g., cars, trucks, motorcycles) without specifying the exact implementation details. We can use the Factory design pattern as follows:\n",
       "\n",
       "```markdown\n",
       "# Vehicle.java (Factory Interface)\n",
       "\n",
       "public interface Vehicle {\n",
       "    void drive();\n",
       "}\n",
       "```\n",
       "\n",
       "```markdown\n",
       "# Car.java (Product Class)\n",
       "\n",
       "public class Car implements Vehicle {\n",
       "    @Override\n",
       "    public void drive() {\n",
       "        System.out.println(\"Driving a car\");\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "```markdown\n",
       "# Truck.java (Product Class)\n",
       "\n",
       "public class Truck implements Vehicle {\n",
       "    @Override\n",
       "    public void drive() {\n",
       "        System.out.println(\"Driving a truck\");\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "```markdown\n",
       "# Motorcycle.java (Product Class)\n",
       "\n",
       "public class Motorcycle implements Vehicle {\n",
       "    @Override\n",
       "    public void drive() {\n",
       "        System.out.println(\"Driving a motorcycle\");\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "```markdown\n",
       "# VehicleFactory.java (Concrete Factory)\n",
       "\n",
       "public class VehicleFactory {\n",
       "    public static Vehicle createVehicle(String type) {\n",
       "        if (type.equals(\"car\")) {\n",
       "            return new Car();\n",
       "        } else if (type.equals(\"truck\")) {\n",
       "            return new Truck();\n",
       "        } else if (type.equals(\"motorcycle\")) {\n",
       "            return new Motorcycle();\n",
       "        } else {\n",
       "            throw new RuntimeException(\"Unsupported vehicle type\");\n",
       "        }\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "```markdown\n",
       "# ClientCode.java\n",
       "\n",
       "public class Main {\n",
       "    public static void main(String[] args) {\n",
       "        Vehicle car = VehicleFactory.createVehicle(\"car\");\n",
       "        car.drive(); // Output: Driving a car\n",
       "\n",
       "        Vehicle truck = VehicleFactory.createVehicle(\"truck\");\n",
       "        truck.drive(); // Output: Driving a truck\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "In this example:\n",
       "\n",
       "*   The `Vehicle` interface defines the contract for all vehicles.\n",
       "*   The `Car`, `Truck`, and `Motorcycle` classes implement the `Vehicle` interface to provide specific implementations.\n",
       "*   The `VehicleFactory` class implements the factory interface, providing a way to create objects without specifying their concrete class.\n",
       "*   The client code uses the `VehicleFactory` to create vehicles of different types without knowing their exact implementation details.\n",
       "\n",
       "**Benefits**\n",
       "------------\n",
       "\n",
       "The Factory design pattern provides several benefits:\n",
       "\n",
       "*   **Decoupling**: It decouples object creation from specific implementations, making it easier to add new functionality or change existing behavior.\n",
       "*   **Flexibility**: It allows for the creation of objects with different properties and behaviors without modifying existing code.\n",
       "*   **Reusability**: It promotes reusing code across multiple contexts by providing a standardized interface for object creation.\n",
       "\n",
       "By using the Factory design pattern, you can create more flexible, maintainable, and scalable software systems that are easier to extend and modify over time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_llama(\"What is a factory in terms of design patterns of OOPS programming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cd7ec-c8ee-46ae-97ea-666250e4bb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
