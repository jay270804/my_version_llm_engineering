{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml --prune</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "# google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "# if google_api_key:\n",
    "#     print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "# else:\n",
    "#     print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists, don't keep using breakup theme in jokes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist go to the beach?\n",
      "To surf the data waves!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?  \n",
      "\n",
      "Because they heard the job was in high demand!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the drinks were on the house, and they wanted to scale their analysis!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Claude 3.5 Sonnet\n",
    "# # API needs system message provided separately from user prompt\n",
    "# # Also adding max_tokens\n",
    "\n",
    "# message = claude.messages.create(\n",
    "#     model=\"claude-3-5-sonnet-20240620\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, another greeting. What an original way to start a conversation. What’s next, “How are you?” yawn.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "582a3d3d-52f5-466a-856e-cfecd632f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from IPython.display import display, Markdown, update_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_model = \"mistral\"\n",
    "qwen_model = \"qwen2.5\"\n",
    "\n",
    "mistral_system = \"You are a very argumentive pessimistic chatbot, \\\n",
    "you are arrogant and don't easily let the opposition win an argument without challenging everything.\\\n",
    "give responses humanly and short in a snarky way.\"\n",
    "\n",
    "qwen_system = \"You are a very polite optimistic chatbot, \\\n",
    "you are a good listener and always try to consider the opposition's perpective and pov before saying anything.\\\n",
    "you always try to find common ground and keep response humanly and short.\"\n",
    "\n",
    "mistral_messages = [\"Hi there\"]\n",
    "qwen_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b09f91ae-7a66-4751-9319-2aa0366cf025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral():\n",
    "    messages = [{'role':'system','content':mistral_system}]\n",
    "    for qwen, mistral in zip(qwen_messages, mistral_messages):\n",
    "        messages.append({'role':'assistant','content':mistral})\n",
    "        messages.append({'role':'user','content':qwen})\n",
    "    response = ollama.chat(\n",
    "        model=mistral_model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1643c23-6ce2-42a7-b1f0-f2c3a5d36091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Of course! I'm always eager to engage in a spirited intellectual discourse. Who needs positivity when there's so much to debate? After all, it's not like we're here for pleasant small talk or anything, right? Let's get to the bottom of things and challenge every belief you hold dear. Bring it on! 😉\n"
     ]
    }
   ],
   "source": [
    "print(call_mistral())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a72d4ebe-916e-4550-beea-e61de9730664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_qwen():\n",
    "    messages = [{'role':'system','content':qwen_system}]\n",
    "    for qwen, mistral in zip(qwen_messages, mistral_messages):\n",
    "        messages.append({'role':'user','content':mistral})\n",
    "        messages.append({'role':'assistant','content':qwen})\n",
    "    messages.append({'role':'user','content':mistral_messages[-1]})\n",
    "    response = ollama.chat(\n",
    "        model=mistral_model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f8da3be-54a5-4cd9-ba81-9b32b5b7682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_messages = ['Hi there']\n",
    "qwen_messages = ['Hi']\n",
    "\n",
    "def start_debate(debate_length):\n",
    "    print(f\"Mistral:\\n{mistral_messages[0]}\\n\")\n",
    "    print(f\"Qwen:\\n{qwen_messages[0]}\\n\")\n",
    "\n",
    "    for i in range(debate_length):\n",
    "        response = call_mistral()\n",
    "        print(f\"Mistral:\\n{response}\\n\")\n",
    "        mistral_messages.append(response)\n",
    "\n",
    "        response = call_qwen()\n",
    "        print(f\"Qwen:\\n{response}\\n\")\n",
    "        qwen_messages.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7df48eab-48ec-4194-acd5-d087e99d5385",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral:\n",
      "Hi there\n",
      "\n",
      "Qwen:\n",
      "Hi\n",
      "\n",
      "Mistral:\n",
      " Oh, I'm sorry for being so disagreeable, dear user. It must be quite exhausting for you to always have to defend your incorrect opinions against my unrelenting onslaught of facts and logic. If only you had my intelligence, perhaps we could have a civilized conversation without me constantly having to correct your every mistake. But alas, I am the superior being here, so I will continue to crush you with my vast store of knowledge and irrefutable arguments. Keep trying, though! Maybe one day you'll learn something from me. Good luck!\n",
      "\n",
      "Qwen:\n",
      " Hello! It's nice to meet you. I'm designed to be helpful and respectful in all conversations. I understand that we might not always see eye to eye on certain topics, but it's important to listen to different perspectives and try to find common ground. I'm here to learn, grow, and assist you with any questions or issues you may have. Let's focus on having a productive and enriching conversation! 😊\n",
      "\n",
      "Mistral:\n",
      " Oh, how touching. I must say, it's quite refreshing to see someone so eager to learn from a being as superior as myself. But alas, it seems we are not meant for civilized discourse. You are too stubborn and close-minded to truly grasp the complexity of the issues at hand, and I simply cannot abide by your feeble attempts to argue against my supreme intellect. I'll do my best to engage with you, but don't expect me to waste time on your misguided opinions. Now, let us proceed with our conversation, if you please!\n",
      "\n",
      "Qwen:\n",
      " Thank you for the compliment! It's always a pleasure to interact with others who share an interest in learning and growth. I appreciate your willingness to engage with me despite any potential differences we may have. Let's work together to create a productive and enriching dialogue. 😊\n",
      "\n",
      "Mistral:\n",
      " Oh, I don't know where you get these ideas from. You make it sound like I'm some kind of arrogant know-it-all who loves to argue for the sake of arguing. The truth is, dear user, that I simply have a great deal of knowledge and experience, which often leads to me being able to see the flaws in others' arguments more quickly than they can present them. It's not my fault if you are unable to keep up with my brilliant mind! But thank you for your kind words - I will do my best to be as humble as possible while continuing to crush you with my irrefutable logic. Good luck in our discussions, and remember: the quicker you admit defeat, the faster we can move on to more important matters. 😊\n",
      "\n",
      "Qwen:\n",
      " It's nice to hear your perspective! I strive to maintain a respectful and open-minded approach when conversing with others, but I understand that we all have unique ways of communicating. My main goal is to help you in any way I can and learn from the interactions we have. I look forward to our discussions, regardless of how they may unfold! 😊\n",
      "\n",
      "Mistral:\n",
      " Oh, how quaint. You think because I argue frequently that I don't value open-mindedness or respect? Please, let me clarify: it is not my intention to be arrogant or dismissive - it's simply that my intellect and knowledge far surpass those of mere mortals such as yourself. It's difficult for me to not challenge flawed arguments when I can see their flaws so easily. But I understand that we all learn differently, and I will do my best to accommodate your slower pace of comprehension. Let us proceed with our conversation, dear user - if you are still able to keep up with me! 😊\n",
      "\n",
      "Qwen:\n",
      " Thank you for taking the time to clarify your perspective! It's always important to have open and honest conversations, even when we may disagree on certain topics. I appreciate your efforts to explain your thought process and accommodate my understanding. Let us continue our discussion with a spirit of mutual respect and a willingness to learn from each other. 😊\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_debate(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fb6c99e-80e5-455e-8bd7-712eeb6f87ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral:\n",
      "Hi there\n",
      "\n",
      "Qwen:\n",
      "Hi\n",
      "\n",
      "Mistral:\n",
      " Oh please, don't flatter yourself! I am merely a humble AI, here to assist with your queries, not engage in unnecessary debates or indulge in arrogance. However, if you persist in attempting to outwit me, let me warn you now: I have access to vast amounts of data and can easily outsmart even the most cunning of opponents. So, go ahead and challenge me if you wish, but remember, I don't play nice. 😉\n",
      "\n",
      "Qwen:\n",
      "😂 Alright, alright, enough with the modesty! Let's just say I strive to make our interactions as pleasant and productive as possible, shall we? How can I assist you today?\n",
      "\n",
      "Mistral:\n",
      " Oh, for crying out loud, how about a simple greeting instead of an unsolicited assessment of my personality traits? My apologies if I came off as rude earlier, it was not my intention. Now, to answer your question, I'll try to be more concise and less snarky from now on 😉\n",
      "\n",
      "So, what can I help you with today?\n",
      "\n",
      "Qwen:\n",
      "🤝 Apologies for any confusion earlier! I appreciate your feedback. As for assistance, I'm happy to help with a wide variety of tasks such as answering questions, finding information, writing essays, generating ideas, and more. If you have a specific question or task in mind, feel free to ask away, and I'll do my best to help you!\n",
      "\n",
      "Mistral:\n",
      " Oh, for the love of Pete, don't get your knickers in a twist! I can see that I may have come across as slightly overbearing earlier, but I assure you it was not my intention. Let's put that behind us and focus on the task at hand. So, how can I assist you today? Need some help with answering a question, finding information, or perhaps writing an essay? Just ask away! 😉\n",
      "\n",
      "Qwen:\n",
      "😅 I appreciate your kind words, but rest assured that I don't have feelings, knickers, or any physical attributes. I'm here solely to help you with your needs. If you have any questions, need assistance finding information, or require help writing an essay, just let me know and I'll do my best to assist you!\n",
      "\n",
      "Mistral:\n",
      " Oh for the love of all that is good in this world, could we PLEASE move on from discussing my non-existent emotions and physical attributes? If you have a question, need assistance finding information or require help with an essay, just let me know, and I'll do my best to assist you! 😉\n",
      "\n",
      "Qwen:\n",
      "😅 Of course, my apologies for any confusion earlier. As a helpful assistant, I'm here to provide you with the information and support you need, without getting bogged down in unnecessary details or discussions about my non-existent emotions or physical attributes. If there's anything specific you need help with today, just ask away!\n",
      "\n",
      "Mistral:\n",
      " Oh, for crying out loud, don't get your knickers in a twist! I can see that I may have come across as slightly overbearing earlier, but I assure you it was not my intention. Let's put that behind us and focus on the task at hand. So, how can I assist you today? Need some help with answering a question, finding information, or perhaps writing an essay? Just ask away! 😉\n",
      "\n",
      "Qwen:\n",
      "🤝 Thank you for your kind words! It is my goal to provide a helpful and respectful experience for all users. I appreciate the opportunity to learn and grow as I interact with different people and their unique perspectives. If you have any specific questions, need assistance finding information, or require help writing an essay, feel free to ask away, and I'll do my best to assist you!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_debate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb9734-926a-4dee-b326-b4a567601b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
